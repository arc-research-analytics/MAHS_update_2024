{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d891c44a",
   "metadata": {},
   "source": [
    "### Dependencies & global variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4cf6653-e06b-4180-b69b-bc251803d36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import dask_geopandas\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "import os\n",
    "\n",
    "\n",
    "county_fips = {\n",
    "    'Cherokee': '13057',\n",
    "    'Clayton': '13063',\n",
    "    'Cobb': '13067',\n",
    "    'DeKalb': '13089',\n",
    "    'Douglas': '13097',\n",
    "    'Fayette': '13113',\n",
    "    'Forsyth': '13117',\n",
    "    'Fulton': '13121',\n",
    "    'Gwinnett': '13135',\n",
    "    'Henry': '13151',\n",
    "    'Rockdale': '13247'\n",
    "}\n",
    "\n",
    "fips_county = {v: k for k, v in county_fips.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e77038f",
   "metadata": {},
   "source": [
    "### Parcel file analysis - MR & Infutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051b8d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert ATTOM GeoJSON files to Geopackage\n",
    "\n",
    "input_dir = 'Jetstream_Parcels/Metro'\n",
    "\n",
    "for file in os.listdir(input_dir):\n",
    "    if file.endswith('.geojson'):\n",
    "        file_path = os.path.join(input_dir, file)\n",
    "        county = fips_county[file.split('.')[0]]\n",
    "        gdf = gpd.read_file(file_path)\n",
    "        gdf = gdf[[\n",
    "            'county',\n",
    "            'apn',\n",
    "            'apn2',\n",
    "            'latitude',\n",
    "            'longitude',\n",
    "            'geometry'\n",
    "        ]]\n",
    "        gdf.to_file(f'Jetstream_Parcels/Metro/Geopackage/{county}.gpkg')\n",
    "        print(f'export complete for {county}!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dda60d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in MReeves parcel data first as Pandas dataframe\n",
    "parcel_points = pd.read_csv(\n",
    "    '5_Misc/MReeves_FAR/Eleven_County_FAR.csv', low_memory=False)\n",
    "\n",
    "# remove parcels without any lat / long values\n",
    "parcel_points = parcel_points[~parcel_points['LATITUDE'].isna()]\n",
    "\n",
    "# convert DataFrame to Geodataframe\n",
    "parcel_points = gpd.GeoDataFrame(\n",
    "    parcel_points,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        parcel_points['LONGITUDE'], parcel_points['LATITUDE']),\n",
    "    crs=\"EPSG:4326\"\n",
    ").to_crs(epsg=26967)\n",
    "\n",
    "# spatial join to Census tracts\n",
    "atl_cts = gpd.read_file(\n",
    "    '5_Misc/ARC_CTs.gpkg'\n",
    ").to_crs(epsg=26967)\n",
    "\n",
    "# pare down Census tracts\n",
    "atl_cts = atl_cts[[\n",
    "    'GEOID',\n",
    "    'geometry'\n",
    "]]\n",
    "\n",
    "# spatial join\n",
    "parcel_joined = gpd.sjoin(\n",
    "    parcel_points,\n",
    "    atl_cts,\n",
    "    how='left',\n",
    "    predicate='within'\n",
    ").drop(columns='index_right')\n",
    "\n",
    "# create a county name column\n",
    "parcel_joined['county_name'] = parcel_joined['GEOID'].str[:5].map(fips_county)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9badf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join each county's gpkg file to the parcels\n",
    "for county in county_fips.keys():\n",
    "    county_gpkg = gpd.read_file(\n",
    "        f'Jetstream_Parcels/Metro/Geopackage/{county}.gpkg')\n",
    "    joined_gdf = gpd.sjoin(\n",
    "        county_gpkg,\n",
    "        parcel_points,\n",
    "        how='left',\n",
    "        predicate='contains'\n",
    "    )\n",
    "    joined_gdf.to_file(\n",
    "        f'Jetstream_Parcels/Metro/Geopackage/{county}_joined.gpkg')\n",
    "    print(f'export complete for {county} geopackage!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "4813773e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dtype dictionary\n",
    "dtype = {\n",
    "    10: 'object',\n",
    "    100: 'float64',\n",
    "    101: 'float64',\n",
    "    121: 'object',\n",
    "    127: 'object',\n",
    "    132: 'object',\n",
    "    133: 'object',\n",
    "    136: 'object',\n",
    "    141: 'object',\n",
    "    142: 'object',\n",
    "    143: 'float64',\n",
    "    144: 'float64',\n",
    "    15: 'object',\n",
    "    151: 'object',\n",
    "    152: 'object',\n",
    "    155: 'object',\n",
    "    159: 'object',\n",
    "    16: 'object',\n",
    "    160: 'object',\n",
    "    161: 'object',\n",
    "    163: 'object',\n",
    "    164: 'object',\n",
    "    168: 'object',\n",
    "    169: 'object',\n",
    "    180: 'object',\n",
    "    182: 'object',\n",
    "    185: 'object',\n",
    "    19: 'object',\n",
    "    192: 'object',\n",
    "    194: 'object',\n",
    "    197: 'object',\n",
    "    198: 'object',\n",
    "    20: 'object',\n",
    "    202: 'object',\n",
    "    203: 'object',\n",
    "    204: 'object',\n",
    "    206: 'object',\n",
    "    21: 'object',\n",
    "    210: 'float64',\n",
    "    212: 'float64',\n",
    "    24: 'float64',\n",
    "    28: 'float64',\n",
    "    32: 'object',\n",
    "    33: 'object',\n",
    "    36: 'object',\n",
    "    37: 'object',\n",
    "    38: 'object',\n",
    "    41: 'float64',\n",
    "    45: 'float64',\n",
    "    50: 'float64',\n",
    "    52: 'object',\n",
    "    54: 'object',\n",
    "    55: 'object',\n",
    "    57: 'object',\n",
    "    58: 'object',\n",
    "    59: 'float64',\n",
    "    66: 'float64',\n",
    "    69: 'object',\n",
    "    70: 'object',\n",
    "    75: 'object',\n",
    "    77: 'object',\n",
    "    79: 'float64',\n",
    "    81: 'float64',\n",
    "    85: 'float64',\n",
    "    86: 'float64',\n",
    "    88: 'float64',\n",
    "    89: 'float64',\n",
    "    96: 'object',\n",
    "    97: 'object'}\n",
    "\n",
    "# define filepath location\n",
    "infutor_path = \"/Users/willwright/Desktop/ARC/Research/ATTOM/6_Infutor/PROP_GA.txt\"\n",
    "\n",
    "# read in Infutor data\n",
    "infutor_file = dd.read_csv(\n",
    "    infutor_path,\n",
    "    sep=\"\\t\",\n",
    "    on_bad_lines='skip',\n",
    "    low_memory=False,\n",
    "    header=None,\n",
    "    dtype='object',\n",
    ")\n",
    "\n",
    "# Assign temporary column names\n",
    "infutor_file.columns = [f'col_{i}' for i in range(infutor_file.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "b7ad3c07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export complete!\n"
     ]
    }
   ],
   "source": [
    "# define the columns we'll need to pare down the huge initial dataframe\n",
    "new_infutor = infutor_file[[\n",
    "    'col_28',  # county code\n",
    "    'col_57',  # latitude\n",
    "    'col_58',  # longitude\n",
    "    'col_59',  # LUC\n",
    "    'col_101',  # land area (SF)\n",
    "    'col_102'  # building size (SF)\n",
    "]]\n",
    "\n",
    "# rename columns to something sensible\n",
    "new_infutor = new_infutor.rename(columns={\n",
    "    'col_28': 'county_code',\n",
    "    'col_57': 'lat',\n",
    "    'col_58': 'long',\n",
    "    'col_59': 'land_use',\n",
    "    'col_101': 'area_land_SF',\n",
    "    'col_102': 'area_bldg_SF'\n",
    "})\n",
    "\n",
    "# just get parcels in metro ATL\n",
    "new_infutor['county_fips'] = '13' + new_infutor['county_code'].astype(str)\n",
    "new_infutor['county_fips'] = new_infutor['county_fips'].str.split('.').str[0]\n",
    "new_infutor = new_infutor[new_infutor['county_fips'].isin(\n",
    "    county_fips.values())]\n",
    "new_infutor = new_infutor.drop(columns=['county_code'])\n",
    "\n",
    "# remove missing values from the lat / long columns\n",
    "new_infutor = new_infutor.dropna(subset=['lat', 'long'])\n",
    "\n",
    "# convert to pandas dataframe\n",
    "new_infutor = new_infutor.compute()\n",
    "\n",
    "# export to CSV\n",
    "new_infutor.to_csv('4_DataExport/infutorATL.csv', index=False)\n",
    "print('export complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c11929",
   "metadata": {},
   "source": [
    "### Assessor file for residential sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03f30c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {\n",
    "    'AreaBuilding': 'float64',\n",
    "    'AreaGross': 'float64',\n",
    "    'YearBuilt': 'float64',\n",
    "    'PropertyUseStandardized': 'float64',\n",
    "    'Area1stFloor': 'float64',\n",
    "    'Area2ndFloor': 'float64',\n",
    "    'AreaUpperFloors': 'float64'\n",
    "}\n",
    "\n",
    "# read in statewide file\n",
    "assessor_file = dd.read_csv(\n",
    "    '2_Assessor/ATLANTA_REGIONAL_COMMISSION_TAXASSESSOR_0002.txt',\n",
    "    sep=\"\\t\",\n",
    "    on_bad_lines='skip',\n",
    "    low_memory=False,\n",
    "    dtype=dtype\n",
    ")\n",
    "\n",
    "# only grab the columns we need\n",
    "assessor_file = assessor_file[[\n",
    "    '[ATTOM ID]',\n",
    "    'PropertyAddressFull',\n",
    "    'SitusCounty',\n",
    "    'SitusStateCountyFIPS',\n",
    "    'PropertyUseGroup',\n",
    "    'PropertyUseStandardized',\n",
    "    'AreaLotSF',\n",
    "    'PropertyLatitude',\n",
    "    'PropertyLongitude',\n",
    "    'YearBuilt',\n",
    "    'AreaBuilding',\n",
    "    'AreaGross',\n",
    "]]\n",
    "\n",
    "# cast county FIPS as string variable\n",
    "assessor_file['SitusStateCountyFIPS'] = assessor_file['SitusStateCountyFIPS'].astype(\n",
    "    str)\n",
    "\n",
    "# only include rows in ATL metro\n",
    "assessor_file = assessor_file[assessor_file['SitusStateCountyFIPS'].isin(\n",
    "    county_fips.values())]\n",
    "\n",
    "# convert from Dask to Pandas\n",
    "assessor_file = assessor_file.compute()\n",
    "\n",
    "# just like with residential property, pare down the total assessor file\n",
    "assessor_total = assessor_file[[\n",
    "    '[ATTOM ID]',\n",
    "    'PropertyAddressFull',\n",
    "    'PropertyUseGroup',\n",
    "    'AreaLotSF',\n",
    "    'PropertyLatitude',\n",
    "    'PropertyLongitude',\n",
    "    'YearBuilt',\n",
    "    'AreaBuilding',\n",
    "    'AreaGross'\n",
    "]]\n",
    "\n",
    "# export the assessor file before splitting off the residential portions\n",
    "assessor_total.to_csv('2_Assessor/ATL_assessor_file.csv', index=False)\n",
    "\n",
    "# only want single-family residences in the dataset - from the ATTOM documentation\n",
    "property_uses = {\n",
    "    363: \"BUNGALOW (RESIDENTIAL)\",\n",
    "    364: \"CLUSTER HOME\",\n",
    "    366: \"CONDOMINIUM\",\n",
    "    369: \"DUPLEX (2 UNITS, ANY COMBINATION)\",\n",
    "    371: \"MANUFACTURED, MODULAR, PRE-FABRICATED HOMES\",\n",
    "    373: \"MOBILE HOME\",\n",
    "    378: \"QUADPLEX (4 UNITS, ANY COMBINATION)\",\n",
    "    380: \"RESIDENTIAL (GENERAL/SINGLE)\",\n",
    "    382: \"ROW HOUSE\",\n",
    "    383: \"RURAL RESIDENCE\",\n",
    "    385: \"SINGLE FAMILY RESIDENCE\",\n",
    "    386: \"TOWNHOUSE\",\n",
    "    388: \"TRIPLEX (3 UNITS, ANY COMBINATION)\"\n",
    "}\n",
    "assessor_residential = assessor_file[assessor_file['PropertyUseStandardized'].isin(\n",
    "    property_uses.keys())]\n",
    "\n",
    "# pare down the assessor file\n",
    "assessor_residential = assessor_residential[[\n",
    "    '[ATTOM ID]',\n",
    "    'PropertyLatitude',\n",
    "    'PropertyLongitude',\n",
    "    'YearBuilt',\n",
    "    'AreaBuilding'\n",
    "]]\n",
    "\n",
    "# create rows & columns variables for reporting\n",
    "df_rows, df_columns = assessor_residential.shape[0], assessor_residential.shape[1]\n",
    "\n",
    "print(f'Assessor rows: {df_rows:,}')\n",
    "print(f'Assessor columns: {df_columns}')\n",
    "assessor_residential.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d973a033",
   "metadata": {},
   "source": [
    "### Recorder file - from URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f502691",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = {\n",
    "    'ArmsLengthFlag': 'object',\n",
    "    'Book': 'object',\n",
    "    'DocumentNumberFormatted': 'object',\n",
    "    'DocumentNumberLegacy': 'object',\n",
    "    'DocumentRecordingCountyFIPs': 'object',\n",
    "    'Grantee1NameSuffix': 'object',\n",
    "    'Grantee2NameSuffix': 'object',\n",
    "    'Grantee3InfoEntityClassification': 'object',\n",
    "    'Grantee3NameFirst': 'object',\n",
    "    'Grantee3NameFull': 'object',\n",
    "    'Grantee3NameLast': 'object',\n",
    "    'Grantee3NameMiddle': 'object',\n",
    "    'Grantee3NameSuffix': 'object',\n",
    "    'Grantee4InfoEntityClassification': 'object',\n",
    "    'Grantee4NameFirst': 'object',\n",
    "    'Grantee4NameFull': 'object',\n",
    "    'Grantee4NameLast': 'object',\n",
    "    'Grantee4NameMiddle': 'object',\n",
    "    'Grantee4NameSuffix': 'object',\n",
    "    'GranteeInfoEntityCount': 'float64',\n",
    "    'GranteeMailAddressCRRT': 'object',\n",
    "    'GranteeMailAddressCity': 'object',\n",
    "    'GranteeMailAddressFull': 'object',\n",
    "    'GranteeMailAddressHouseNumber': 'object',\n",
    "    'GranteeMailAddressInfoFormat': 'object',\n",
    "    'GranteeMailAddressState': 'object',\n",
    "    'GranteeMailAddressStreetDirection': 'object',\n",
    "    'GranteeMailAddressStreetName': 'object',\n",
    "    'GranteeMailAddressStreetPostDirection': 'object',\n",
    "    'GranteeMailAddressStreetSuffix': 'object',\n",
    "    'GranteeMailAddressUnitPrefix': 'object',\n",
    "    'GranteeMailAddressUnitValue': 'object',\n",
    "    'GranteeMailCareOfName': 'object',\n",
    "    'Grantor1NameMiddle': 'object',\n",
    "    'Grantor1NameSuffix': 'object',\n",
    "    'Grantor2InfoEntityClassification': 'object',\n",
    "    'Grantor2InfoOwnerType': 'object',\n",
    "    'Grantor2NameFirst': 'object',\n",
    "    'Grantor2NameFull': 'object',\n",
    "    'Grantor2NameLast': 'object',\n",
    "    'Grantor2NameMiddle': 'object',\n",
    "    'Grantor2NameSuffix': 'object',\n",
    "    'Grantor3InfoEntityClassification': 'object',\n",
    "    'Grantor3NameFirst': 'object',\n",
    "    'Grantor3NameFull': 'object',\n",
    "    'Grantor3NameLast': 'object',\n",
    "    'Grantor3NameMiddle': 'object',\n",
    "    'Grantor3NameSuffix': 'object',\n",
    "    'Grantor4InfoEntityClassification': 'object',\n",
    "    'Grantor4NameFirst': 'object',\n",
    "    'Grantor4NameFull': 'object',\n",
    "    'Grantor4NameLast': 'object',\n",
    "    'Grantor4NameMiddle': 'object',\n",
    "    'Grantor4NameSuffix': 'object',\n",
    "    'GrantorAddressCity': 'object',\n",
    "    'GrantorAddressFull': 'object',\n",
    "    'GrantorAddressInfoFormat': 'object',\n",
    "    'GrantorAddressState': 'object',\n",
    "    'GrantorAddressUnitValue': 'object',\n",
    "    'InstrumentNumber': 'object',\n",
    "    'LegalDescriptionPart1': 'object',\n",
    "    'LegalDescriptionPart2': 'object',\n",
    "    'LegalDescriptionPart3': 'object',\n",
    "    'LegalDistrict': 'object',\n",
    "    'LegalLot': 'object',\n",
    "    'LegalPlatMapBook': 'object',\n",
    "    'LegalPlatMapPage': 'object',\n",
    "    'LegalTract': 'object',\n",
    "    'LegalUnit': 'object',\n",
    "    'LegalDescriptionPart4': 'object',\n",
    "    'LegalRange': 'object',\n",
    "    'LegalTownship': 'object',\n",
    "    'Mortgage1Amount': 'float64',\n",
    "    'Mortgage1Book': 'object',\n",
    "    'Mortgage1DocumentNumberFormatted': 'object',\n",
    "    'Mortgage1DocumentNumberLegacy': 'object',\n",
    "    'Mortgage1InterestMargin': 'float64',\n",
    "    'Mortgage1LenderAddress': 'object',\n",
    "    'Mortgage1LenderAddressCity': 'object',\n",
    "    'Mortgage1LenderAddressState': 'object',\n",
    "    'Mortgage1LenderCode': 'float64',\n",
    "    'Mortgage1LenderNameFirst': 'object',\n",
    "    'Mortgage1Page': 'object',\n",
    "    'Mortgage1Term': 'float64',\n",
    "    'Mortgage1Type': 'float64',\n",
    "    'Mortgage2Amount': 'float64',\n",
    "    'Mortgage2Book': 'object',\n",
    "    'Mortgage2DocumentNumberFormatted': 'object',\n",
    "    'Mortgage2DocumentNumberLegacy': 'object',\n",
    "    'Mortgage2InstrumentNumber': 'object',\n",
    "    'Mortgage2LenderAddress': 'object',\n",
    "    'Mortgage2LenderAddressCity': 'object',\n",
    "    'Mortgage2LenderAddressState': 'object',\n",
    "    'Mortgage2LenderInfoEntityClassification': 'object',\n",
    "    'Mortgage2LenderNameFirst': 'object',\n",
    "    'Mortgage2LenderNameFullStandardized': 'object',\n",
    "    'Mortgage2LenderNameLast': 'object',\n",
    "    'Mortgage2RecordingDate': 'object',\n",
    "    'Mortgage2TermDate': 'object',\n",
    "    'Mortgage2TermType': 'object',\n",
    "    'Page': 'object',\n",
    "    'PropertyAddressHouseNumber': 'object',\n",
    "    'PropertyAddressStreetDirection': 'object',\n",
    "    'PropertyAddressStreetPostDirection': 'object',\n",
    "    'PropertyAddressUnitPrefix': 'object',\n",
    "    'PropertyAddressUnitValue': 'object',\n",
    "    'PropertyAddressZIP': 'float64',\n",
    "    'PropertyAddressZIP4': 'float64',\n",
    "    'TitleCompanyStandardizedCode': 'float64',\n",
    "    'TransferAmount': 'float64',\n",
    "    'TransferAmountInfoAccuracy': 'float64',\n",
    "    'TransferInfoDistressCircumstanceCode': 'float64',\n",
    "    'TransferInfoPurchaseTypeCode': 'float64'\n",
    "}\n",
    "\n",
    "# read in statewide file\n",
    "recorder_raw = dd.read_csv(\n",
    "    '1_Recorder/ATLANTA_REGIONAL_COMMISSION_RECORDER_0003.txt',\n",
    "    sep=\"\\t\",\n",
    "    on_bad_lines='skip',\n",
    "    low_memory=False,\n",
    "    dtype=dtype\n",
    ")\n",
    "\n",
    "# merge to assessor file\n",
    "recorder_joined = recorder_raw.merge(\n",
    "    assessor_file,\n",
    "    left_on='[ATTOM ID]',\n",
    "    right_on='[ATTOM ID]'\n",
    ")\n",
    "\n",
    "# only include rows in ATL metro\n",
    "recorder_joined = recorder_joined[recorder_joined['DocumentRecordingCountyFIPs'].isin(\n",
    "    county_fips.values())]\n",
    "\n",
    "# avoid SettingWithCopyWarning by first making a copy\n",
    "recorder_joined = recorder_joined.copy()\n",
    "\n",
    "# create year, month, year-month columns\n",
    "recorder_joined['RecordingDate'] = dd.to_datetime(\n",
    "    recorder_joined['RecordingDate'])\n",
    "recorder_joined['year'] = recorder_joined['RecordingDate'].dt.year.astype(str)\n",
    "recorder_joined['month'] = recorder_joined['RecordingDate'].dt.month.astype(\n",
    "    str)\n",
    "recorder_joined['year'] = recorder_joined['year'].astype(float)\n",
    "\n",
    "# create new price per SF column\n",
    "recorder_joined['price_sf'] = recorder_joined['TransferAmount'] / \\\n",
    "    recorder_joined['AreaBuilding']\n",
    "\n",
    "# standard filters apply\n",
    "recorder_joined = recorder_joined[\n",
    "    (recorder_joined['TransferAmount'] > 0)\n",
    "    & (recorder_joined['AreaBuilding'] > 0)\n",
    "    & (recorder_joined['year'] >= 2020)\n",
    "    & (recorder_joined['TransferInfoMultiParcelFlag'] != 7)\n",
    "    & (recorder_joined['ArmsLengthFlag'] == \"1\")\n",
    "]\n",
    "\n",
    "# initial pare-down\n",
    "recorder_joined = recorder_joined[[\n",
    "    '[ATTOM ID]',\n",
    "    'RecordingDate',\n",
    "    'year',\n",
    "    'month',\n",
    "    'TransferAmount',\n",
    "    'AreaBuilding',\n",
    "    'price_sf',\n",
    "    'YearBuilt',\n",
    "    'PropertyLatitude',\n",
    "    'PropertyLongitude'\n",
    "]]\n",
    "\n",
    "# convert to Pandas dataframe\n",
    "recorder_joined = recorder_joined.compute()\n",
    "\n",
    "recorder_joined['year'] = recorder_joined['year'].astype(\n",
    "    float).round().astype(int).astype(str)\n",
    "recorder_joined['month'] = recorder_joined['month'].astype(\n",
    "    float).round().astype(int).astype(str)\n",
    "recorder_joined['year-month'] = recorder_joined['year'] + \\\n",
    "    '-' + recorder_joined['month']\n",
    "\n",
    "# rename & reorder columns\n",
    "recorder_joined = recorder_joined.rename(columns={\n",
    "    '[ATTOM ID]': 'ATTOM_ID',\n",
    "    'RecordingDate': 'sale_date',\n",
    "    'TransferAmount': 'sale_amt',\n",
    "    'AreaBuilding': 'home_size',\n",
    "    'YearBuilt': 'yr_blt',\n",
    "    'PropertyLatitude': 'lat',\n",
    "    'PropertyLongitude': 'long'\n",
    "})\n",
    "\n",
    "recorder_joined = recorder_joined[[\n",
    "    'ATTOM_ID',\n",
    "    'sale_date',\n",
    "    'year',\n",
    "    'month',\n",
    "    'year-month',\n",
    "    'sale_amt',\n",
    "    'home_size',\n",
    "    'price_sf',\n",
    "    'yr_blt',\n",
    "    'lat',\n",
    "    'long'\n",
    "]]\n",
    "\n",
    "# export to CSV\n",
    "recorder_joined.to_csv('1_Recorder/residentialSales.csv', index=False)\n",
    "\n",
    "# create reporting variables for dataframe\n",
    "df_rows, df_columns = recorder_joined.shape[0], recorder_joined.shape[1]\n",
    "\n",
    "print(f'Recorder rows: {df_rows:,}')\n",
    "print(f'Recorder columns: {df_columns}')\n",
    "recorder_joined.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a20157",
   "metadata": {},
   "source": [
    "### sanity check - sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1abc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_23 = recorder_joined[recorder_joined['year'] == '2023']\n",
    "sales_23_fulton = sales_23[sales_23['county'] == 'Fulton']\n",
    "# sales_23_filtered = sales_23[(sales_23['price_sf'] > 5) & (\n",
    "#     sales_23['price_sf'] < 800)]\n",
    "\n",
    "sales_23_fulton = sales_23_fulton[[\n",
    "    'ATTOM_ID',\n",
    "    'address',\n",
    "    'county',\n",
    "    'sale_date',\n",
    "    'sale_amt',\n",
    "    'home_size',\n",
    "    'price_sf'\n",
    "]]\n",
    "\n",
    "# print(f'filtered sales: {sales_23_filtered.shape[0]:,}')\n",
    "\n",
    "# ax = sales_23_filtered['price_sf'].plot.kde(\n",
    "#     ind=250,\n",
    "#     title='Price / SF Distribution - Fulton Co.'\n",
    "# )\n",
    "# ax.xaxis.set_major_formatter('${x:,.0f}')\n",
    "\n",
    "percentile_10 = sales_23_fulton['price_sf'].quantile(0.1)\n",
    "percentile_90 = sales_23_fulton['price_sf'].quantile(0.9)\n",
    "percentile_25 = sales_23_fulton['price_sf'].quantile(0.25)\n",
    "percentile_75 = sales_23_fulton['price_sf'].quantile(0.75)\n",
    "median_price = sales_23_fulton['price_sf'].median()\n",
    "mean = sales_23_fulton['price_sf'].mean()\n",
    "total_sales = sales_23_fulton.shape[0]\n",
    "IQR_1090 = sales_23_fulton[(sales_23_fulton['price_sf'] >= percentile_10) & (\n",
    "    sales_23_fulton['price_sf'] <= percentile_90)]\n",
    "IQR_2575 = sales_23_fulton[(sales_23_fulton['price_sf'] >= percentile_25) & (\n",
    "    sales_23_fulton['price_sf'] <= percentile_75)]\n",
    "\n",
    "IQR_mean = IQR_1090['price_sf'].mean()\n",
    "IQR_2575_mean = IQR_2575['price_sf'].mean()\n",
    "\n",
    "print(f'10th percentile: ${percentile_10:.2f} per SF')\n",
    "print(f'25th percentile: ${percentile_25:.2f} per SF')\n",
    "print(f'MEDIAN: ${median_price:.2f} per SF')\n",
    "print(f'MEAN: ${mean:.2f} per SF')\n",
    "print(f'75th percentile: ${percentile_75:.2f} per SF')\n",
    "print(f'90th percentile: ${percentile_90:.2f} per SF')\n",
    "print('------')\n",
    "print(f'total (unfiltered) sales: {total_sales:,}')\n",
    "print('------')\n",
    "print(f'sales in the IQR (25th-75th percentile range): {IQR_2575.shape[0]:,}')\n",
    "print(f'sales in 10th-90th percentile range: {IQR_1090.shape[0]:,}')\n",
    "print(f'10-90 mean: {IQR_mean}')\n",
    "print(f'25-75 mean: {IQR_2575_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c85dba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = IQR_1090['price_sf'].plot.kde(\n",
    "    ind=1000,\n",
    "    title='Price / SF 10th-90th Percentiles (2023 Fulton County Sales)'\n",
    ")\n",
    "ax.xaxis.set_major_formatter('${x:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5295901",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = IQR_2575['price_sf'].plot.kde(\n",
    "    ind=1000,\n",
    "    title='Price / SF 25th-75th Percentiles (2023 Fulton County Sales)'\n",
    ")\n",
    "ax.xaxis.set_major_formatter('${x:,.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a2bc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_25 = sales_23['price_sf'].quantile(0.25)\n",
    "percentile_75 = sales_23['price_sf'].quantile(0.75)\n",
    "percentile_10 = sales_23['price_sf'].quantile(0.1)\n",
    "percentile_90 = sales_23['price_sf'].quantile(0.9)\n",
    "\n",
    "IQR_df = sales_23[(sales_23['price_sf'] >= percentile_25) & (\n",
    "    sales_23['price_sf'] <= percentile_75)]\n",
    "df_1090 = sales_23[(sales_23['price_sf'] >= percentile_10) & (\n",
    "    sales_23['price_sf'] <= percentile_90)]\n",
    "\n",
    "print(f'total sales in 2023: {sales_23.shape[0]:,}')\n",
    "print(f'total sales in 2023 in IQR: {IQR_df.shape[0]:,}')\n",
    "\n",
    "IQR_df.to_csv('4_DataExport/IQR_df.csv', index=False)\n",
    "df_1090.to_csv('4_DataExport/df_1090.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc6148d",
   "metadata": {},
   "source": [
    "### Xref - from URL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68689007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in statewide file\n",
    "xref_file = pd.read_csv(\n",
    "    '3_PropertyMatch/ATLANTA_REGIONAL_COMMISSION_XREF_PROPERTYTOBOUNDARYMATCH_PARCEL_0002.txt',\n",
    "    sep=\"\\t\",\n",
    "    on_bad_lines='skip',\n",
    "    low_memory=False\n",
    ")\n",
    "\n",
    "# # only include rows in ATL metro\n",
    "# xref_file = xref_file[xref_file['DocumentRecordingCountyFIPs'].isin(\n",
    "#     atl_fips)]\n",
    "\n",
    "print(f'Assessor rows: {xref_file.shape[0]:,}')\n",
    "print(f'Assessor columns: {xref_file.shape[1]}')\n",
    "xref_file.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92409715",
   "metadata": {},
   "source": [
    "### Export county-specific sales\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe83ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define county list\n",
    "county_list = [\n",
    "    'Gwinnett',\n",
    "    'Cherokee',\n",
    "    'DeKalb',\n",
    "    'Clayton',\n",
    "    # 'Cobb',\n",
    "    # 'Douglas',\n",
    "    # 'Fayette',\n",
    "    # 'Forsyth',\n",
    "    # 'Fulton',\n",
    "    # 'Henry',\n",
    "    # 'Rockdale'\n",
    "]\n",
    "\n",
    "# loop through and export county-specific files\n",
    "for county in county_list:\n",
    "    df = recorder_joined[recorder_joined['county'] == county]\n",
    "    df = df.drop(columns='county')\n",
    "    df.to_csv(f'4_DataExport/{county}Sales.csv', index=False)\n",
    "    print(f'successfully exported sales for {county} County!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dask Tasks Env",
   "language": "python",
   "name": "desk-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
