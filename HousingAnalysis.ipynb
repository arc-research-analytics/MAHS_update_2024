{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p> This notebook goes through the process of using a supervised learning algorithm to assign classification labels to Census tracts in the Atlanta region. Next, the code spatially indexes and then aggregates home sales from 2018 and 2023 to the tract level, joining these data with other metrics at the same geography.</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Dependencies & global variables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# define list of metro ATL counties\n",
    "atl_FIPS = {\n",
    "    'Cherokee': '13057',\n",
    "    'Clayton': '13063',\n",
    "    'Cobb': '13067',\n",
    "    'DeKalb': '13089',\n",
    "    'Douglas': '13097',\n",
    "    'Fayette': '13113',\n",
    "    'Forsyth': '13117',\n",
    "    'Fulton': '13121',\n",
    "    'Gwinnett': '13135',\n",
    "    'Henry': '13151',\n",
    "    'Rockdale': '13247'\n",
    "}\n",
    "\n",
    "# create inverse dictionary\n",
    "inverse_FIPS_dict = {value: key for key, value in atl_FIPS.items()}\n",
    "\n",
    "# pre-define the urban counties on which to run the supervised learning model\n",
    "urban_counties = ['13063', '13121', '13089', '13135', '13067']\n",
    "\n",
    "# pre-define rural counties, which are determined by Census designation (i.e., no supervised learning model)\n",
    "rural_counties = ['13057', '13097', '13113', '13117', '13151', '13247']\n",
    "\n",
    "# Load 2020 tract geometries into memory via GeoDataFrames\n",
    "url_tracts_2020 = \"https://www2.census.gov/geo/tiger/TIGER2024/TRACT/tl_2024_13_tract.zip\"\n",
    "GA_2020_Tracts = gpd.read_file(url_tracts_2020).to_crs(epsg=26916)\n",
    "GA_2020_Tracts['FIPS'] = GA_2020_Tracts['STATEFP'] + GA_2020_Tracts['COUNTYFP']\n",
    "GA_2020_Tracts = GA_2020_Tracts[[\n",
    "    'GEOID',\n",
    "    'FIPS',\n",
    "    'geometry'\n",
    "]]\n",
    "GA_2020_Tracts = GA_2020_Tracts[GA_2020_Tracts['FIPS'].isin(atl_FIPS.values())]\n",
    "\n",
    "# Load 2010 tract geometries into memory via GeoDataFrames\n",
    "url_tracts_2010 = \"https://www2.census.gov/geo/tiger/TIGER2018/TRACT/tl_2018_13_tract.zip\"\n",
    "GA_2010_Tracts = gpd.read_file(url_tracts_2010).to_crs(epsg=26916)\n",
    "GA_2010_Tracts['FIPS'] = GA_2010_Tracts['STATEFP'] + GA_2010_Tracts['COUNTYFP']\n",
    "GA_2010_Tracts = GA_2010_Tracts[[\n",
    "    'FIPS',\n",
    "    'GEOID',\n",
    "    'geometry'\n",
    "]]\n",
    "GA_2010_Tracts = GA_2010_Tracts[GA_2010_Tracts['FIPS'].isin(atl_FIPS.values())]\n",
    "\n",
    "# Read in land use classification labels (urban-suburban-rural) used previously for ML model\n",
    "landUse_labels = pd.read_csv(\n",
    "    'Data/tracts_by_submarket.csv',\n",
    "    dtype={\n",
    "        'GEOID': 'str'\n",
    "    })\n",
    "# should be in the format of TRACT_ID [string] | Submarket [string]\n",
    "\n",
    "# Read in tract-level land use percentages for 2018\n",
    "landUse_proportions_2018 = pd.read_csv(\n",
    "    'Data/LandUse_ByTract_2018.csv', dtype={'GEOID': 'str'})\n",
    "# should be in the structure of TRACT_ID [string] | Ag_LandUsePercentage [float] \\ Comm_LandUsePercentage [float]...\n",
    "\n",
    "# Read in tract-level land use percentages for 2023\n",
    "landUse_proportions_2023 = pd.read_csv(\n",
    "    'Data/LandUse_ByTract_2023.csv', dtype={'GEOID': 'str'})\n",
    "# same structure as above\n",
    "\n",
    "# Define the keywords to include, remove from the LandUse_ByTract files above\n",
    "# Certain land uses may adversely affect the model and may need to be filtered out\n",
    "land_uses = [\n",
    "    'GEOID',\n",
    "    'Commercial Office',\n",
    "    'Commercial Office - 2+ Floors',\n",
    "    'Commercial Office - Mixed Use',\n",
    "    'Commercial Retail',\n",
    "    # 'Commercial Retail - Condomium',\n",
    "    'Commercial Retail - Department Store',\n",
    "    'Commercial Retail - Food & Drink',\n",
    "    # 'Commercial Retail - Gas and Service Station',\n",
    "    # 'Commercial Retail - Hotel, Motel, Resort',\n",
    "    # 'Commercial Retail - Liquor, Convenience Store',\n",
    "    'Commercial Retail - Mall',\n",
    "    # 'Commercial Retail - Park Lot, Garage',\n",
    "    'Commercial Retail - Shopping Plaza, Mini-Mall',\n",
    "    'Commercial Retail - Truck Stop',\n",
    "    # 'Commercial Retail - Vet, Animal Hospital',\n",
    "    'Multi-Family Residential',\n",
    "    'Multi-Family Residential - 2 to 4 units',\n",
    "    'Multi-Family Residential - 5+ units',\n",
    "    'Multi-Family Residential - High-Rise',\n",
    "    # 'Multi-Family Residential - Mobile Home Park',\n",
    "    'Residential  ',\n",
    "    'Residential - Condo',\n",
    "    # 'Residential - Mobile Home Park',\n",
    "    # 'Residential - Parking Garage',\n",
    "    'Residential - Townhouse'\n",
    "]\n",
    "\n",
    "# Select filtered columns to make dataframe with only residential & commercial\n",
    "landUse_proportions_2018 = landUse_proportions_2018[land_uses]\n",
    "landUse_proportions_2023 = landUse_proportions_2023[land_uses]\n",
    "\n",
    "# Read in the rural tracts as designated by Census\n",
    "percent_rural = pd.read_csv('Data/percent_rural.csv', dtype={'GEOID': 'str'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Calculate distance to urban core for both training & testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows in 2018 dataset: 782\n",
      "rows in 2023 dataset: 1,247\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GEOID</th>\n",
       "      <th>Commercial Office</th>\n",
       "      <th>Commercial Office - 2+ Floors</th>\n",
       "      <th>Commercial Office - Mixed Use</th>\n",
       "      <th>Commercial Retail</th>\n",
       "      <th>Commercial Retail - Department Store</th>\n",
       "      <th>Commercial Retail - Food &amp; Drink</th>\n",
       "      <th>Commercial Retail - Mall</th>\n",
       "      <th>Commercial Retail - Shopping Plaza, Mini-Mall</th>\n",
       "      <th>Commercial Retail - Truck Stop</th>\n",
       "      <th>Multi-Family Residential</th>\n",
       "      <th>Multi-Family Residential - 2 to 4 units</th>\n",
       "      <th>Multi-Family Residential - 5+ units</th>\n",
       "      <th>Multi-Family Residential - High-Rise</th>\n",
       "      <th>Residential</th>\n",
       "      <th>Residential - Condo</th>\n",
       "      <th>Residential - Townhouse</th>\n",
       "      <th>distance_from_core</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13057090101</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.012453</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003113</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.893524</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>65619.565436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13057090102</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.941067</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>64648.339127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13057090103</td>\n",
       "      <td>0.005302</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.843054</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025451</td>\n",
       "      <td>61638.095114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         GEOID  Commercial Office  Commercial Office - 2+ Floors  \\\n",
       "0  13057090101           0.004981                            0.0   \n",
       "1  13057090102           0.001856                            0.0   \n",
       "2  13057090103           0.005302                            0.0   \n",
       "\n",
       "   Commercial Office - Mixed Use  Commercial Retail  \\\n",
       "0                            0.0           0.012453   \n",
       "1                            0.0           0.001856   \n",
       "2                            0.0           0.015907   \n",
       "\n",
       "   Commercial Retail - Department Store  Commercial Retail - Food & Drink  \\\n",
       "0                                   0.0                          0.003113   \n",
       "1                                   0.0                          0.000000   \n",
       "2                                   0.0                          0.001060   \n",
       "\n",
       "   Commercial Retail - Mall  Commercial Retail - Shopping Plaza, Mini-Mall  \\\n",
       "0                  0.000623                                            0.0   \n",
       "1                  0.000000                                            0.0   \n",
       "2                  0.000000                                            0.0   \n",
       "\n",
       "   Commercial Retail - Truck Stop  Multi-Family Residential  \\\n",
       "0                             0.0                       0.0   \n",
       "1                             0.0                       0.0   \n",
       "2                             0.0                       0.0   \n",
       "\n",
       "   Multi-Family Residential - 2 to 4 units  \\\n",
       "0                                 0.001245   \n",
       "1                                 0.000000   \n",
       "2                                 0.001060   \n",
       "\n",
       "   Multi-Family Residential - 5+ units  Multi-Family Residential - High-Rise  \\\n",
       "0                                  0.0                                   0.0   \n",
       "1                                  0.0                                   0.0   \n",
       "2                                  0.0                                   0.0   \n",
       "\n",
       "   Residential    Residential - Condo  Residential - Townhouse  \\\n",
       "0       0.893524                  0.0                 0.000000   \n",
       "1       0.941067                  0.0                 0.000000   \n",
       "2       0.843054                  0.0                 0.025451   \n",
       "\n",
       "   distance_from_core  \n",
       "0        65619.565436  \n",
       "1        64648.339127  \n",
       "2        61638.095114  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to calculate distance from a reference point\n",
    "def calculate_distance_to_core(tracts, core_point_projected):\n",
    "    tracts['centroid'] = tracts.centroid\n",
    "    tracts['distance_from_core'] = tracts['centroid'].distance(\n",
    "        core_point_projected)\n",
    "    return tracts\n",
    "\n",
    "\n",
    "# Function to merge datasets and clean up columns\n",
    "def merge_and_clean_data(land_use_df, tracts_df, drop_columns, key='GEOID'):\n",
    "    merged_df = pd.merge(land_use_df, tracts_df, how='left', on=key)\n",
    "    merged_df = merged_df.drop(columns=drop_columns)\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "# Set lat / long values to some point in CBD (in this case, downtown Atlanta)\n",
    "core_lat, core_lon = 33.76, -84.39\n",
    "core_point = Point(core_lon, core_lat)\n",
    "core_point_projected = gpd.GeoSeries(\n",
    "    [core_point], crs=\"EPSG:4326\").to_crs(epsg=26916).iloc[0]\n",
    "\n",
    "# Drop unneeded columns\n",
    "drop_columns = ['FIPS', 'geometry', 'centroid']\n",
    "\n",
    "# Process 2018 dataset\n",
    "GA_2010_Tracts = calculate_distance_to_core(\n",
    "    GA_2010_Tracts, core_point_projected)\n",
    "distance_df_2018 = merge_and_clean_data(\n",
    "    landUse_proportions_2018, GA_2010_Tracts, drop_columns)\n",
    "\n",
    "# Process 2023 dataset\n",
    "GA_2020_Tracts = calculate_distance_to_core(\n",
    "    GA_2020_Tracts, core_point_projected)\n",
    "distance_df_2023 = merge_and_clean_data(\n",
    "    landUse_proportions_2023, GA_2020_Tracts, drop_columns)\n",
    "\n",
    "# Print results\n",
    "print(f'rows in 2018 dataset: {distance_df_2018.shape[0]:,}')\n",
    "print(f'rows in 2023 dataset: {distance_df_2023.shape[0]:,}')\n",
    "distance_df_2023.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - X, Y splits -> train -> run model for urban counties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export complete!\n"
     ]
    }
   ],
   "source": [
    "urban_dfs = []\n",
    "\n",
    "# will run the model on just the urban counties as defined above\n",
    "for county in urban_counties:\n",
    "\n",
    "    # filter 2018 data\n",
    "    groupby = distance_df_2018[distance_df_2018['GEOID'].str.startswith(\n",
    "        county)]\n",
    "\n",
    "    merged_df_labels = pd.merge(\n",
    "        groupby,\n",
    "        landUse_labels,\n",
    "        how='inner',\n",
    "        on='GEOID'\n",
    "    )\n",
    "\n",
    "    # clean up the dataframe\n",
    "    merged_df_labels = merged_df_labels.drop(columns='Submarket')\n",
    "\n",
    "    # move the classification column from the last to the second position\n",
    "    new_order = merged_df_labels.columns.tolist()\n",
    "    new_order.insert(1, new_order.pop())\n",
    "    merged_df_labels = merged_df_labels.reindex(columns=new_order)\n",
    "\n",
    "    # rename label column\n",
    "    merged_df_labels = merged_df_labels.rename(columns={\n",
    "        'General Landuse Type': 'label'\n",
    "    })\n",
    "\n",
    "    # remove rural designation from the test dataset\n",
    "    merged_df_labels = merged_df_labels[merged_df_labels['label'] != 'Rural']\n",
    "\n",
    "    # Separate the features and labels\n",
    "    X = merged_df_labels.drop(columns=['GEOID', 'label'])\n",
    "    y = merged_df_labels['label']\n",
    "\n",
    "    # define a scaler, use it to fit & transform\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # save scaler as pickle file\n",
    "    joblib.dump(\n",
    "        scaler, f'Pickle/scaler_{inverse_FIPS_dict[county].lower()}.pkl')\n",
    "\n",
    "    # split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled,\n",
    "        y,\n",
    "        test_size=0.2,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Train the model with the best parameters\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion='entropy',\n",
    "        splitter='random',\n",
    "        max_features=16,\n",
    "        random_state=1\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save the trained model to .pkl\n",
    "    joblib.dump(\n",
    "        model,\n",
    "        f'Pickle/decision_tree_model_{inverse_FIPS_dict[county].lower()}.pkl'\n",
    "    )\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # prediction time for 2023 data ----------------------------------------------------\n",
    "\n",
    "    # load saved pickle files\n",
    "    scaler = joblib.load(\n",
    "        f'Pickle/scaler_{inverse_FIPS_dict[county].lower()}.pkl')\n",
    "    model = joblib.load(\n",
    "        f'Pickle/decision_tree_model_{inverse_FIPS_dict[county].lower()}.pkl')\n",
    "\n",
    "    # filter 2023 data\n",
    "    filtered_2023_data = distance_df_2023[distance_df_2023['GEOID'].str.startswith(\n",
    "        county)]\n",
    "\n",
    "    # drop the tract ID column so we are only left with land use proportions\n",
    "    X_new = filtered_2023_data.drop(columns='GEOID')\n",
    "\n",
    "    # scale to the new 2023 data\n",
    "    X_new_scaled = scaler.transform(X_new)\n",
    "\n",
    "    # Predict using the loaded model\n",
    "    y_new_pred = model.predict(X_new_scaled)\n",
    "\n",
    "    # add predictions\n",
    "    model_df_23 = filtered_2023_data.copy()\n",
    "    model_df_23['label'] = y_new_pred\n",
    "\n",
    "    # slim down the dataframe\n",
    "    model_df_23 = model_df_23[[\n",
    "        'GEOID',\n",
    "        'label'\n",
    "    ]]\n",
    "\n",
    "    # Add this dataframe to the 'urban_dfs' dataframe\n",
    "    urban_dfs.append(model_df_23)\n",
    "\n",
    "combined_df = pd.concat(urban_dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Label all tracts with proper designations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in the rest of metro tracts to 'combined_df'\n",
    "new_geoids = GA_2020_Tracts[~GA_2020_Tracts['GEOID'].isin(\n",
    "    combined_df['GEOID'])]\n",
    "new_rows = new_geoids.copy()\n",
    "new_rows['label'] = 'TBD'\n",
    "new_rows = new_rows[[\n",
    "    'GEOID',\n",
    "    'label'\n",
    "]]\n",
    "\n",
    "# Append the new rows to combined_df\n",
    "decisiontree_combined = pd.concat([combined_df, new_rows], ignore_index=True)\n",
    "\n",
    "# join the percent rural to this dataframe and override the label if rural > 50\n",
    "decisiontree_combined = decisiontree_combined.merge(\n",
    "    percent_rural,\n",
    "    how='left',\n",
    "    on='GEOID'\n",
    ").drop(columns=['ALAND', 'Total Urban Area'])\n",
    "\n",
    "# Step 1: Update 'label' to \"Rural\" where 'Percent Rural' is >= 50\n",
    "decisiontree_combined.loc[decisiontree_combined['Percent Rural']\n",
    "                          >= 50, 'label'] = 'Rural'\n",
    "\n",
    "# Step 2: Change remaining \"TBD\" values in 'label' to \"Suburban\"\n",
    "decisiontree_combined.loc[decisiontree_combined['label']\n",
    "                          == 'TBD', 'label'] = 'Suburban'\n",
    "\n",
    "# Add geometryðŸ‘‡; stays the same for all techniques\n",
    "gdf_DecisionTree_urban = gpd.GeoDataFrame(pd.merge(\n",
    "    decisiontree_combined,\n",
    "    GA_2020_Tracts,\n",
    "    how='left',\n",
    "    left_on='GEOID',\n",
    "    right_on='GEOID'\n",
    "))\n",
    "gdf_DecisionTree_urban\n",
    "\n",
    "\n",
    "# smoothing technique for suburban tracts --------------------------------------------\n",
    "# Function to calculate the shared boundary length\n",
    "def shared_boundary_length(suburban_geom, urban_sindex, urban_tracts):\n",
    "    possible_matches_index = list(\n",
    "        urban_sindex.intersection(suburban_geom.bounds))\n",
    "    possible_matches = urban_tracts.iloc[possible_matches_index]\n",
    "    possible_matches = possible_matches[possible_matches.intersects(\n",
    "        suburban_geom)]\n",
    "    shared_length = sum(possible_matches.intersection(suburban_geom).length)\n",
    "    return shared_length\n",
    "\n",
    "\n",
    "# Smoothing function\n",
    "def smooth_labels(gdf):\n",
    "    # Reproject to a suitable projected CRS (NAD83 / Georgia West)\n",
    "    gdf = gdf.to_crs(epsg=2240)\n",
    "\n",
    "    suburban_tracts = gdf[gdf['label'] == 'Suburban'].copy()\n",
    "    urban_tracts = gdf[gdf['label'] == 'Urban'].copy()\n",
    "\n",
    "    # Create spatial index for urban tracts for faster querying\n",
    "    urban_sindex = urban_tracts.sindex\n",
    "\n",
    "    # Calculate the shared boundary length for each suburban tract\n",
    "    suburban_tracts['shared_length'] = suburban_tracts.geometry.apply(\n",
    "        lambda geom: shared_boundary_length(geom, urban_sindex, urban_tracts))\n",
    "\n",
    "    # Calculate the total boundary length of each suburban tract\n",
    "    suburban_tracts['total_length'] = suburban_tracts.geometry.length\n",
    "\n",
    "    # Calculate the percentage of boundary touching urban tracts\n",
    "    suburban_tracts['percentage_touching_urban'] = suburban_tracts['shared_length'] / \\\n",
    "        suburban_tracts['total_length']\n",
    "\n",
    "    # Update the label for tracts where more than 50% of the boundary touches urban tracts\n",
    "    suburban_tracts.loc[suburban_tracts['percentage_touching_urban']\n",
    "                        > 0.5, 'label'] = 'Urban'\n",
    "\n",
    "    # Merge back the updated suburban tracts with the original GeoDataFrame\n",
    "    gdf.update(suburban_tracts)\n",
    "\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# function that will run the smoothing process a specified number of times\n",
    "def run_smoothing(gdf, iterations):\n",
    "    for i in range(iterations):\n",
    "        gdf = smooth_labels(gdf)\n",
    "    return gdf\n",
    "\n",
    "\n",
    "# Number of times to run the smoothing function\n",
    "iterations = 2\n",
    "\n",
    "# Run the smoothing function\n",
    "gdf_DecisionTree_urban = run_smoothing(gdf_DecisionTree_urban, iterations)\n",
    "\n",
    "# end of tract smoothing technique--------------------------------------------------\n",
    "\n",
    "# Slim down & export\n",
    "gdf_DecisionTree_urban_export = gdf_DecisionTree_urban[[\n",
    "    'GEOID',\n",
    "    'label'\n",
    "]]\n",
    "gdf_DecisionTree_urban_export.to_csv('Data/tracts_with_labels.csv')\n",
    "print('export complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Process home sales data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in unaggregated sales\n",
    "sales = pd.read_csv(f'../4_DataExport/residentialSales_Updated.csv')\n",
    "\n",
    "# Convert DataFrame to GeoDataFrame so we can spatially join\n",
    "sales = gpd.GeoDataFrame(\n",
    "    sales,\n",
    "    geometry=gpd.points_from_xy(\n",
    "        sales['long'], sales['lat']),\n",
    "    crs=\"EPSG:4269\"\n",
    ").to_crs(epsg=26967)\n",
    "\n",
    "# spatially join sales to geography of choice\n",
    "sales = gpd.sjoin(\n",
    "    sales,\n",
    "    GA_2020_Tracts,\n",
    "    how='left',\n",
    "    predicate='within'\n",
    ").drop(columns='index_right')\n",
    "\n",
    "# if a sale didn't join to a Census tract, drop it\n",
    "sales = sales[sales['GEOID'].notna()]\n",
    "\n",
    "# using the GEOID column, create a 'county' column\n",
    "sales['county'] = sales['GEOID'].str[:5].map(inverse_FIPS_dict)\n",
    "\n",
    "# pare down & reorder sales\n",
    "sales = sales[[\n",
    "    'ATTOM_ID',  # keeping this column so we can remove duplicate sales of the same property\n",
    "    'county',\n",
    "    'GEOID',\n",
    "    'sale_date',\n",
    "    'year',\n",
    "    'month',\n",
    "    'sale_amt',\n",
    "    'price_sf',\n",
    "    'home_size',\n",
    "    'yr_blt',\n",
    "    'res_type',\n",
    "    'lat',\n",
    "    'long'\n",
    "]]\n",
    "\n",
    "# Group by both ATTOM_ID and year to count occurrences for each group\n",
    "grouped_counts_df = sales.groupby(\n",
    "    ['ATTOM_ID', 'year']).size().reset_index(name='sale_count')\n",
    "\n",
    "# Merge the grouped counts back into the original 'sales' DataFrame\n",
    "sales = sales.merge(grouped_counts_df, on=['ATTOM_ID', 'year'], how='left')\n",
    "\n",
    "\n",
    "# define function to compute the cleaned price / SF, which will calculate an \"aggregated\" price / sf if a property has sold more than once in a given year\n",
    "def clean_price_sf(row):\n",
    "    attom_id = row['ATTOM_ID']\n",
    "    year = row['year']\n",
    "    count = row['sale_count']\n",
    "\n",
    "    if count == 1:\n",
    "        return row['price_sf']\n",
    "    else:\n",
    "        subset = sales[(sales['ATTOM_ID'] == attom_id)\n",
    "                       & (sales['year'] == year)]\n",
    "        if count <= 3:\n",
    "            return subset['price_sf'].max()\n",
    "        else:\n",
    "            return subset['price_sf'].median()\n",
    "\n",
    "\n",
    "# Apply the de-dupe function defined above to create the 'price_sf_cleaned' column\n",
    "sales = sales.copy()\n",
    "sales['price_sf_cleaned'] = sales.apply(clean_price_sf, axis=1)\n",
    "\n",
    "# Now that duplicate properties share a common price / SF, we can drop any dupes (while keeping the first instance)\n",
    "sales = sales.sort_values(by=['ATTOM_ID', 'year'])\n",
    "sales = sales.drop_duplicates(subset=['ATTOM_ID', 'year'], keep='first')\n",
    "\n",
    "\n",
    "# 2-step to drop the original 'price_sf' column and then rename the cleaned 'price_sf' back to its original name\n",
    "sales = sales.drop(columns='price_sf').rename(columns={\n",
    "    'price_sf_cleaned': 'price_sf'\n",
    "})\n",
    "\n",
    "# map residential type descriptions\n",
    "res_type_dict = {\n",
    "    385.0: 'single-family residence',\n",
    "    383.0: 'single-family residence',\n",
    "    380.0: 'single-family residence',\n",
    "    363.0: 'single-family residence',\n",
    "    366.0: 'condominium',\n",
    "    386.0: 'townhouse',\n",
    "    382.0: 'townhouse',\n",
    "    369.0: 'small multi-family',\n",
    "    388.0: 'small multi-family',\n",
    "    378.0: 'small multi-family',\n",
    "    373.0: 'mobile home',\n",
    "    371.0: 'modular',\n",
    "    364.0: 'other'\n",
    "}\n",
    "\n",
    "# create new column with short descriptions\n",
    "sales['res_type_desc'] = sales['res_type'].map(res_type_dict).fillna('unknown')\n",
    "\n",
    "# drop the numeric 3-digit residential type column\n",
    "sales = sales.drop(columns='res_type')\n",
    "\n",
    "# For comparison's sake, this will create a tract-level rollup without any filters applied\n",
    "sales_23 = sales[sales['year'] == 2023]\n",
    "sales_18 = sales[sales['year'] == 2018]\n",
    "sales_years = list(sales['year'].unique())\n",
    "\n",
    "\n",
    "# define the modified z-score outliers function\n",
    "def filter_by_modified_zscore(data, threshold=3.5):\n",
    "    median = np.median(data)\n",
    "    mad = np.median(np.abs(data - median))\n",
    "    if mad == 0:\n",
    "        return data  # No outliers if mad is 0\n",
    "    modified_z_scores = 0.6745 * (data - median) / mad\n",
    "    return data[np.abs(modified_z_scores) < threshold]\n",
    "\n",
    "\n",
    "# define the percentile outliers filter function\n",
    "def filter_by_percentiles(data, lower_percentile, upper_percentile):\n",
    "    lower_bound = np.percentile(data, lower_percentile)\n",
    "    upper_bound = np.percentile(data, upper_percentile)\n",
    "    return data[(data >= lower_bound) & (data <= upper_bound)]\n",
    "\n",
    "\n",
    "# define function to filter data by each method\n",
    "def filter_group(group, method, **kwargs):\n",
    "    prices = group['price_sf']\n",
    "    if method == 'modified_zscore':\n",
    "        filtered_prices = filter_by_modified_zscore(prices, **kwargs)\n",
    "    elif method == 'percentiles':\n",
    "        filtered_prices = filter_by_percentiles(prices, **kwargs)\n",
    "    return group[group['price_sf'].isin(filtered_prices)]\n",
    "\n",
    "\n",
    "# apply the filter function above to the dataset for each year, for each Census tract\n",
    "sales_list = []\n",
    "for year in sales_years:\n",
    "    df_year = sales[sales['year'] == year]\n",
    "\n",
    "    filtered_sales_zscore = df_year.groupby('GEOID', group_keys=False).apply(\n",
    "        filter_group,\n",
    "        method='modified_zscore',\n",
    "        threshold=3.5,\n",
    "        include_groups=True\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    sales_list.append(filtered_sales_zscore)\n",
    "\n",
    "# create cleaned dataframe, inclusive of all years\n",
    "combined_cleaned_full_df = pd.concat(sales_list, ignore_index=False)\n",
    "\n",
    "# create cleaned dataframes of just 2018, 2023\n",
    "filtered_sales_zscore_23 = sales_23.groupby('GEOID').apply(\n",
    "    filter_group,\n",
    "    method='modified_zscore',\n",
    "    threshold=3.5,\n",
    "    include_groups=False\n",
    ").reset_index(drop=False).drop(columns='level_1')\n",
    "filtered_sales_zscore_18 = sales_18.groupby('GEOID').apply(\n",
    "    filter_group,\n",
    "    method='modified_zscore',\n",
    "    threshold=3.5,\n",
    "    include_groups=False\n",
    ").reset_index(drop=False).drop(columns='level_1')\n",
    "\n",
    "# create tract-level summary dataframes\n",
    "sales_23_grouped = filtered_sales_zscore_23.groupby(\n",
    "    'GEOID')['price_sf'].median().reset_index()\n",
    "sales_18_grouped = filtered_sales_zscore_18.groupby(\n",
    "    'GEOID')['price_sf'].median().reset_index()\n",
    "\n",
    "# merge 2018 and 2023 dataframes\n",
    "sales_finalMerge = pd.merge(\n",
    "    sales_18_grouped,\n",
    "    sales_23_grouped,\n",
    "    how='outer',\n",
    "    left_on='GEOID',\n",
    "    right_on='GEOID',\n",
    "    suffixes=('_18', '_23')\n",
    ")\n",
    "\n",
    "# calculate percent change columns (numeric & percentage)\n",
    "sales_finalMerge['5yrChange_num'] = sales_finalMerge['price_sf_23'] - \\\n",
    "    sales_finalMerge['price_sf_18']\n",
    "sales_finalMerge['5yrChange_per'] = (sales_finalMerge['price_sf_23'] -\n",
    "                                     sales_finalMerge['price_sf_18']) / sales_finalMerge['price_sf_18'] * 100\n",
    "\n",
    "\n",
    "sales_finalMerge.to_csv('4_DataExport/submarket_sales_4.csv', index=False)\n",
    "print('export complete!')\n",
    "print(f'rows in 2018 & 2023 dataset: {sales_finalMerge.shape[0]:,}')\n",
    "sales_finalMerge.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Load & merge data sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sales data\n",
    "sales_df = pd.read_csv(\n",
    "    '../4_DataExport/submarket_sales_4.csv',\n",
    "    dtype={\n",
    "        'GEOID': 'str'\n",
    "    })\n",
    "\n",
    "# read in the urban / suburban / rural labels\n",
    "labels_df = pd.read_csv(\n",
    "    'Supervised/tracts_with_labels.csv',\n",
    "    dtype={\n",
    "        'GEOID': 'str'\n",
    "    })\n",
    "\n",
    "# CSV containing job-to-pop ratio\n",
    "jobPop_ratio = pd.read_csv(\n",
    "    'submarket_analysis.csv',\n",
    "    dtype={\n",
    "        'GEOID': 'str'\n",
    "    })\n",
    "\n",
    "jobPop_ratio = jobPop_ratio[[\n",
    "    'GEOID',\n",
    "    'Job_to_pop_ratio'\n",
    "]]\n",
    "\n",
    "# get town centers\n",
    "url = 'https://services1.arcgis.com/Ug5xGQbHsD8zuZzM/arcgis/rest/services/2019_UGPM_WFL1/FeatureServer/4/query?where=1%3D1&objectIds=&time=&geometry=&geometryType=esriGeometryEnvelope&inSR=&spatialRel=esriSpatialRelIntersects&resultType=none&distance=0.0&units=esriSRUnit_Meter&relationParam=&returnGeodetic=false&outFields=&returnGeometry=true&returnCentroid=false&returnEnvelope=false&featureEncoding=esriDefault&multipatchOption=xyFootprint&maxAllowableOffset=&geometryPrecision=&outSR=&defaultSR=&datumTransformation=&applyVCSProjection=false&returnIdsOnly=false&returnUniqueIdsOnly=false&returnCountOnly=false&returnExtentOnly=false&returnQueryGeometry=false&returnDistinctValues=false&cacheHint=false&orderByFields=&groupByFieldsForStatistics=&outStatistics=&having=&resultOffset=&resultRecordCount=&returnZ=false&returnM=false&returnTrueCurves=false&returnExceededLimitFeatures=true&quantizationParameters=&sqlFormat=none&f=pjson&token='\n",
    "\n",
    "town_centers = gpd.read_file(url)\n",
    "town_centers = town_centers.to_crs(GA_2020_Tracts.crs)\n",
    "\n",
    "# Perform a spatial join to find intersections\n",
    "intersected = gpd.sjoin(GA_2020_Tracts, town_centers,\n",
    "                        how='left', predicate='intersects')\n",
    "intersected = intersected.drop_duplicates(subset='GEOID')\n",
    "\n",
    "# Create the 'town_center' column based on whether an intersection was found\n",
    "GA_2020_Tracts['in_town_center'] = intersected['index_right'].notnull()\n",
    "\n",
    "GA_2020_Tracts = GA_2020_Tracts[[\n",
    "    'GEOID',\n",
    "    'in_town_center',\n",
    "]]\n",
    "\n",
    "# merge the 2 above labels\n",
    "final_df1 = pd.merge(\n",
    "    sales_df,\n",
    "    labels_df,\n",
    "    how='right',\n",
    "    on='GEOID'\n",
    ").drop(columns=['5yrChange_num', '5yrChange_per'])\n",
    "\n",
    "# drop any tracts where there was no 2018 or 2023 sales data\n",
    "final_df1 = final_df1[final_df1['price_sf_23'].notna()]\n",
    "final_df1 = final_df1[final_df1['price_sf_18'].notna()]\n",
    "\n",
    "# merge with the job/pop ratio dataframe\n",
    "final_df2 = final_df1.merge(\n",
    "    jobPop_ratio,\n",
    "    how='left',\n",
    "    on='GEOID'\n",
    ")\n",
    "\n",
    "# merge with town center dataframe\n",
    "final_df3 = final_df2.merge(\n",
    "    GA_2020_Tracts,\n",
    "    on='GEOID'\n",
    ")\n",
    "final_df3 = final_df3[[\n",
    "    'GEOID',\n",
    "    'label',\n",
    "    'in_town_center',\n",
    "    'Job_to_pop_ratio',\n",
    "    'price_sf_18',\n",
    "    'price_sf_23'\n",
    "]]\n",
    "\n",
    "summ_df = final_df3.groupby('label')['price_sf_23'].median().reset_index()\n",
    "\n",
    "final_df4 = final_df3.merge(\n",
    "    summ_df,\n",
    "    on='label'\n",
    ").rename(columns={\n",
    "    'price_sf_23_x': 'price_sf_23',\n",
    "    'price_sf_23_y': 'label_median'\n",
    "})\n",
    "\n",
    "final_df4['price_desc'] = final_df4.apply(\n",
    "    lambda row: 'high priced' if row['price_sf_23'] >= row['label_median'] else 'low priced',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "final_df4.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Finally, apply submarket labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create copy\n",
    "final_df5 = final_df4.copy()\n",
    "\n",
    "\n",
    "# Function to calculate percent change when it becomes needed\n",
    "def calculate_percent_change(row):\n",
    "    return ((row['price_sf_23'] - row['price_sf_18']) / row['price_sf_18']) * 100\n",
    "\n",
    "\n",
    "# Calculate percent change for 'Urban' tracts\n",
    "final_df5['percent_change'] = final_df5.apply(lambda row: calculate_percent_change(\n",
    "    row) if row['label'] == 'Urban' else 'N/A', axis=1)\n",
    "\n",
    "# Calculate the median percent change for 'Urban' records\n",
    "median_percent_change = final_df5[final_df5['label']\n",
    "                                  == 'Urban']['percent_change'].median()\n",
    "\n",
    "\n",
    "# Define the function to apply the logic\n",
    "def assign_submarket(row):\n",
    "    if row['label'] == 'Rural':\n",
    "        if row['price_desc'] == 'high priced':\n",
    "            return 10\n",
    "        elif row['price_desc'] == 'low priced':\n",
    "            return 9\n",
    "    elif row['label'] == 'Urban':\n",
    "        if row['price_desc'] == 'high priced':\n",
    "            if row['Job_to_pop_ratio'] >= 1:\n",
    "                return 2\n",
    "            else:\n",
    "                return 1\n",
    "        elif row['price_desc'] == 'low priced':\n",
    "            if row['percent_change'] > median_percent_change:\n",
    "                return 3\n",
    "            else:\n",
    "                return 4\n",
    "    elif row['label'] == 'Suburban':\n",
    "        if (row['Job_to_pop_ratio'] >= 1 or row['in_town_center']) and row['price_desc'] == 'high priced':\n",
    "            return 5\n",
    "        elif (row['Job_to_pop_ratio'] >= 1 or row['in_town_center']) and row['price_desc'] == 'low priced':\n",
    "            return 8\n",
    "        elif not row['in_town_center'] and row['price_desc'] == 'high priced':\n",
    "            return 6\n",
    "        elif not row['in_town_center'] and row['price_desc'] == 'low priced':\n",
    "            return 7\n",
    "    return None  # In case none of the conditions match\n",
    "\n",
    "\n",
    "# Apply the function to create the 'submarket' column\n",
    "final_df5['submarket'] = final_df5.apply(assign_submarket, axis=1)\n",
    "\n",
    "# Drop the temporary 'percent_change' column\n",
    "final_df5.drop(columns=['percent_change'], inplace=True)\n",
    "\n",
    "print(\n",
    "    f'median change in urban tracts from 18 to 23: {median_percent_change:.1f}%')\n",
    "\n",
    "# export to CSV\n",
    "final_df5.to_csv('submarket_by_tract.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
